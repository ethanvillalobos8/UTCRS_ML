{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c708466a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52badff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import chi2\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3d5d2",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494d55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (239487, 141)\n",
      "Grouped data shape: (52706, 6)\n",
      "Merged data shape: (239487, 142)\n",
      "The size of X_1 after sampling is (71725, 19)\n",
      "The size of y1 after sampling is (71725, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "# The dataset is Highway-Rail Grade Crossing Accident dataset\n",
    "# Importing the dataset, Highway-Rail Grade Crossing Accident dataset\n",
    "data = pd.read_csv('Highway-Rail_Grade_Crossing_Accident_Data.csv', low_memory=False)\n",
    "print(\"Original data shape:\", data.shape)\n",
    "\n",
    "# Count accidents by unique company pairs\n",
    "grouped_data = data.groupby(['Railroad Code', 'Incident Year', 'State Name', 'Highway User Position', 'Equipment Involved']).size().reset_index(name='Accident Count')\n",
    "print(\"Grouped data shape:\", grouped_data.shape)\n",
    "\n",
    "# Merge the 'Accident Count' column back into the original data\n",
    "merged_data = pd.merge(data, grouped_data, on=['Railroad Code', 'Incident Year', 'State Name', 'Highway User Position', 'Equipment Involved'], how='left')\n",
    "print(\"Merged data shape:\", merged_data.shape)\n",
    "\n",
    "# Define features and target variable\n",
    "categorical_features = ['Highway User Position', 'Equipment Involved']\n",
    "numeric_features = ['Incident Year']\n",
    "target = 'Accident Count'\n",
    "\n",
    "# Drop rows with missing values in the target variable\n",
    "merged_data.dropna(subset=[target], inplace=True)\n",
    "\n",
    "# Convert 'Incident Year' to full year format\n",
    "merged_data['Incident Year'] = merged_data['Incident Year'].apply(lambda year: 2000 + year if year <= 21 else 1900 + year)\n",
    "\n",
    "# Define X and y\n",
    "X = merged_data[numeric_features + categorical_features]\n",
    "y = merged_data[target]\n",
    "y = np.reshape(y, (len(y), 1))\n",
    "\n",
    "# Convert categorical features to numerical data\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "X_transformed = ct.fit_transform(X).toarray()\n",
    "\n",
    "sample_size = int(X_transformed.shape[0] * 0.3)  \n",
    "\n",
    "rows_id = random.sample(range(X_transformed.shape[0]), sample_size)\n",
    "X_1 = X_transformed[rows_id, :]\n",
    "y1 = y[rows_id]\n",
    "\n",
    "print(\"The size of X_1 after sampling is {}\".format(X_1.shape))\n",
    "print(\"The size of y1 after sampling is {}\".format(y1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec50f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training data is (57380, 19)\n",
      "The size of test data is (14345, 19)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y1, test_size = 0.20, random_state = 0)\n",
    "print(\"The size of training data is {}\".format(X_train.shape))\n",
    "print(\"The size of test data is {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1a2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data (fitting the scaler on the training data only)\n",
    "scaler_X = MinMaxScaler().fit(X_train)\n",
    "scaler_y = MinMaxScaler().fit(y_train)\n",
    "X_train_scaled = scaler_X.transform(X_train)\n",
    "y_train_scaled = scaler_y.transform(y_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e4ffd",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e952c08-4a44-4e21-9c3f-412993baec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = X_train_scaled.T, y_train_scaled.T\n",
    "test_X, test_Y = X_test_scaled.T, y_test_scaled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d3676a-a89b-4de0-9ee9-dc8884e795de",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  # number of worker nodes\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "splits = list(kf.split(X_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bed9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_map(X, L, sigma):\n",
    "    # L:  number of random features\n",
    "    np.random.seed(1)\n",
    "    d,_ = X.shape\n",
    "    #generate multivariate normal distribution\n",
    "    Mu = np.zeros(d)     # zero mean     \n",
    "    Sigma = 1/(sigma**2) * np.eye(d)  # covariance matrix\n",
    "    Omega = np.random.multivariate_normal(Mu, Sigma, L)\n",
    "    b = np.random.uniform(0, 2*np.pi, (L, 1))\n",
    "    Phi_X = math.sqrt(2/L) * np.cos(np.dot(Omega, X) + b)\n",
    "    return Phi_X, Omega, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7745dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(yhat, y):\n",
    "    \"\"\"\n",
    "    yhat is the output from fully connected layer (num_classes*num_examples)\n",
    "    y is labels (num_classes*num_examples, y is one-hot encoded vector)\n",
    "    \"\"\"\n",
    "    m = y.shape[1]  #num_examples\n",
    "    diff  = y - yhat\n",
    "    loss = (1/2/m) * np.sum(np.square(diff))   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb94772-0b28-4258-a255-e378eb9cfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define federated training function\n",
    "def federated_training(X, y, Phi_train, lr, lambd, rounds, splits):\n",
    "    theta_global = np.zeros((Phi_train.shape[0], 1))\n",
    "    for t in range(rounds):\n",
    "        gradient_accumulator = np.zeros_like(theta_global)\n",
    "        for train_index, test_index in splits:\n",
    "            X_local_train, y_local_train = X[:, train_index], y[:, train_index]\n",
    "            X_local_test, y_local_test = X[:, test_index], y[:, test_index]\n",
    "\n",
    "            Phi_local_train = Phi_train[:, train_index]\n",
    "            Phi_local_test = Phi_train[:, test_index]\n",
    "\n",
    "            # Local model update through gradient descent\n",
    "            predictions = np.dot(theta_global.T, Phi_local_train)\n",
    "            error = predictions - y_local_train\n",
    "            gradient = np.dot(Phi_local_train, error.T) / len(train_index) + lambd * theta_global\n",
    "            theta_global -= lr * gradient\n",
    "\n",
    "            # Accumulate gradients\n",
    "            gradient_accumulator += gradient\n",
    "\n",
    "        # Average gradients and update global model\n",
    "        theta_global -= lr * gradient_accumulator / k\n",
    "\n",
    "    return theta_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f7d0aa-f9a7-465a-a3ac-71c447932bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "# # Define the hyperparameter space\n",
    "# space = {\n",
    "#     'lr': hp.uniform('lr', 0.001, 0.1),\n",
    "#     'lambd': hp.loguniform('lambd', np.log(1e-5), np.log(1e-3)),\n",
    "#     'rounds': hp.choice('rounds', range(5, 20)),\n",
    "#     'L': hp.choice('L', range(100, 400)),\n",
    "#     'sigma': hp.choice('sigma', range(1, 6))\n",
    "# }\n",
    "\n",
    "# def objective(params):\n",
    "#     # Extract parameters from the Hyperopt params dictionary\n",
    "#     L = int(params['L'])\n",
    "#     sigma = params['sigma']\n",
    "#     Phi_train, Omega, b = rf_map(train_X, L, sigma)\n",
    "#     Phi_test, Omega_test, b_test = rf_map(test_X, L, sigma)\n",
    "    \n",
    "#     lr = params['lr']\n",
    "#     lambd = params['lambd']\n",
    "#     rounds = int(params['rounds'])\n",
    "#     k = 10  # number of splits\n",
    "#     kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "#     splits = list(kf.split(X_train_scaled))\n",
    "    \n",
    "#     # Perform federated training (assuming federated_training function is defined)\n",
    "#     theta_global = federated_training(train_X, train_Y, Phi_train, lr, lambd, rounds, splits)\n",
    "    \n",
    "#     # Evaluate the model (assuming a suitable evaluation metric function is defined)\n",
    "#     testY_RF = np.dot(theta_global.T, Phi_test)\n",
    "\n",
    "#     # Calculate test MSE and RMSE\n",
    "#     test_mse = compute_cost(testY_RF, test_Y)\n",
    "#     test_rmse = math.sqrt(test_mse)\n",
    "    \n",
    "#     performance_metric = test_rmse\n",
    "\n",
    "#     # Return the loss value and status\n",
    "#     return {'loss': performance_metric, 'status': STATUS_OK}\n",
    "\n",
    "# # Perform Bayesian optimization\n",
    "# trials = Trials()\n",
    "# best_params = fmin(objective, space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5a6f83-2000-4e45-a70f-898e61a8eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF mapping performed once\n",
    "L = 275\n",
    "sigma = 1\n",
    "Phi_train, Omega, b = rf_map(train_X, L, sigma)\n",
    "Phi_test, Omega_test, b_test = rf_map(test_X, L, sigma)\n",
    "\n",
    "# Federated learning parameters\n",
    "lr = 0.09584131766022012\n",
    "lambd = 0.0003648662609891207\n",
    "rounds = 14\n",
    "k = 10  # number of splits\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "splits = list(kf.split(X_train_scaled))\n",
    "\n",
    "# Perform federated training\n",
    "start = time.perf_counter()\n",
    "theta_global = federated_training(train_X, train_Y, Phi_train, lr, lambd, rounds, splits)\n",
    "end = time.perf_counter()\n",
    "trf = end - start\n",
    "# print(\"The process takes %.4f seconds \" %(trf))\n",
    "# print(\"Train cost is \" + str(train_costs_RF[-1]))\n",
    "# print(\"Test MSE is \" + str(test_MSE_RF[-1]))\n",
    "# print(\"Test RMSE is \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdea988-5d32-4ae2-91e2-23ca75583532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the global model using test set\n",
    "testY_RF = np.dot(theta_global.T, Phi_test)\n",
    "\n",
    "# Calculate test MSE and RMSE\n",
    "test_mse = compute_cost(testY_RF, test_Y)\n",
    "test_rmse = math.sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ddecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process takes 6.8368 seconds \n",
      "Test MSE is 0.007540100465111884\n",
      "Test RMSE is 0.08683375187743464\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "print(\"The process takes %.4f seconds \" %(trf))\n",
    "print(f\"Test MSE is {test_mse}\")\n",
    "print(f\"Test RMSE is {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6932c870-a320-42a2-86c6-519d7280753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper main idea\n",
    "\n",
    "# instead of transforming the data into a higher dimensional space, we use random features \n",
    "# that capture the important aspects of the data and transform the data into a smaller \n",
    "# dimensional space to reduce computational complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
